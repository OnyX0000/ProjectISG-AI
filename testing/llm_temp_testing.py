import random
from typing import List, Dict
import matplotlib.pyplot as plt
from langchain_ollama import ChatOllama
from rouge import Rouge
from evaluate import load as load_metric
from bert_score import score as bertscore
import pandas as pd
import gc

# üí° Ïö©ÎèÑÎ≥Ñ ÌîÑÎ°¨ÌîÑÌä∏ ÌÖåÏä§Ìä∏ÏÖã Ï†ïÏùò
PROMPT_SETS = {
    "question": [
        {"input": "Îã§Ïùå ÎåÄÌôî ÎÇ¥Ïö©ÏùÑ Î∞îÌÉïÏúºÎ°ú MBTI ÏßàÎ¨∏ÏùÑ ÏÉùÏÑ±Ìï¥Ï§ò: ÎÇòÎäî ÌòºÏûê ÏûàÎäî Í≤å Ìé∏Ìï¥", "reference": "ÌòºÏûê ÏûàÎäî Í≤ÉÏùÑ Ï¶êÍ∏∞ÏãúÎÇòÏöî, ÏïÑÎãàÎ©¥ ÏÇ¨ÎûåÎì§Í≥º Ìï®Íªò ÏûàÏùÑ Îïå Îçî Ìé∏ÌïúÍ∞ÄÏöî?"},
    ],
    "evaluation": [
        {"input": "ÏßàÎ¨∏: ÌèâÏÜåÏóê Í≥ÑÌöçÏùÑ ÏÑ∏Ïö∞Îäî Í≤ÉÏùÑ Ï¢ãÏïÑÌïòÏãúÎÇòÏöî? ÎåÄÎãµ: Í∑∏ÎïåÍ∑∏Îïå Îã¨ÎùºÏöî.", "reference": "Ìï¥Îãπ ÎåÄÎãµÏùÄ Í≥ÑÌöçÏùÑ ÏÑ∏Ïö∞Îäî Í≤ÉÏùÑ ÏÑ†Ìò∏ÌïòÏßÄ ÏïäÎäî Í≤ΩÌñ•ÏúºÎ°ú Ìï¥ÏÑùÎê©ÎãàÎã§."},
    ],
    "diary": [
        {"input": "2024-05-27Ïùò ÌôúÎèô: ÎÜçÏû• Î¨ºÏ£ºÍ∏∞, ÎßàÏùÑ ÏÇ∞Ï±Ö, ÍΩÉ ÏàòÌôï", "reference": "Ïò§ÎäòÏùÄ ÌèâÌôîÎ°úÏö¥ ÌïòÎ£®ÏòÄÎã§. ÍΩÉÏùÑ ÏàòÌôïÌïòÍ≥† ÎßàÏùÑÏùÑ Í±∞ÎãêÎ©∞ Í≥†ÏöîÌï®ÏùÑ ÎäêÍºàÎã§."},
    ],
    "emotion": [
        {"input": "Ïò§ÎäòÏùò Í∞êÏÑ± ÏùºÍ∏∞: ÌñáÏÇ¥ Í∞ÄÎìùÌïú Î∞≠ÏóêÏÑú ÍΩÉÏùÑ ÎèåÎ≥¥Î©∞ ÌïòÎ£®Î•º Î≥¥ÎÉàÎã§.", "reference": "Í≥†ÏöîÌï®, ÎßåÏ°±Í∞ê"},
    ],
    "sfx": [
        {"input": "ÎÇòÎ≠áÏûéÏù¥ Î∞îÎûåÏóê Ïä§ÏπòÎäî ÏÜåÎ¶¨", "reference": "rustling leaves"},
    ],
    "comfy": [
        {"input": "Î∞§ÌïòÎäò ÏïÑÎûò Ïù¥ÏÑ∏Í≥Ñ ÎßàÏùÑÏùò Ï°∞Ïö©Ìïú Ïßë", "reference": "fantasy village under the stars"},
    ]
}

TEMPERATURES = [0.2, 0.7, 1.0]

MODEL_MAPPING = {
    "question": "gemma3:12b",
    "evaluation": "gemma3:12b",
    "diary": "gemma3:12b",
    "emotion": "gemma3:12b",
    "sfx": "qwen3:8b",
    "comfy": "qwen3:8b"
}

def evaluate_outputs(references: List[str], predictions: List[str]) -> Dict[str, float]:
    rouge = Rouge()
    rouge_scores = rouge.get_scores(predictions, references, avg=True)["rouge-l"]

    # üí° BLEU ÏòàÏô∏ Ï≤òÎ¶¨
    try:
        import evaluate
        bleu = evaluate.load("sacrebleu")
        bleu_result = bleu.compute(predictions=predictions, references=[[r] for r in references])
        bleu_score = bleu_result["score"]
    except Exception as e:
        print(f"‚ùå BLEU Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
        bleu_score = 0.0

    # üí° BERTScore
    bert_p, bert_r, bert_f = bertscore(predictions, references, lang="en")

    return {
        "ROUGE-L": rouge_scores["f"],
        "BLEU": bleu_score,
        "BERTScore-F1": float(bert_f.mean())
    }

def test_llm_outputs() -> pd.DataFrame:
    records = []
    model_cache = {}

    for use_case, prompts in PROMPT_SETS.items():
        references = [p["reference"] for p in prompts]
        input_texts = [p["input"] for p in prompts]

        for temp in TEMPERATURES:
            model_name = MODEL_MAPPING[use_case]
            model_key = f"{model_name}_{temp}"

            if model_key not in model_cache:
                model_cache[model_key] = ChatOllama(model=model_name, temperature=temp)
            llm = model_cache[model_key]

            predictions = [llm.invoke(p).content for p in input_texts]

            metrics = evaluate_outputs(references, predictions)
            metrics["use_case"] = use_case
            metrics["temperature"] = temp
            metrics["model"] = model_name
            records.append(metrics)

        gc.collect()  # GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨

    return pd.DataFrame(records)

def plot_metrics(df: pd.DataFrame):
    metrics = ["ROUGE-L", "BLEU", "BERTScore-F1"]
    fig, axs = plt.subplots(len(metrics), 1, figsize=(10, 12))

    for i, metric in enumerate(metrics):
        for use_case in df["use_case"].unique():
            subset = df[df["use_case"] == use_case]
            axs[i].plot(subset["temperature"], subset[metric], marker='o', label=use_case)
        axs[i].set_title(metric)
        axs[i].set_xlabel("Temperature")
        axs[i].set_ylabel(metric)
        axs[i].legend()
        axs[i].grid(True)

    plt.tight_layout()
    plt.savefig("llm_evaluation_summary.png")
    print("‚úÖ ÌèâÍ∞Ä ÏôÑÎ£å: llm_evaluation_summary.png Ï†ÄÏû•Îê®")

if __name__ == "__main__":
    df_result = test_llm_outputs()
    plot_metrics(df_result)

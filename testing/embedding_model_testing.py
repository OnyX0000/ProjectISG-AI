import numpy as np
import gc
import pandas as pd
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer
from langchain_ollama import OllamaEmbeddings
from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import spearmanr, pearsonr

try:
    import torch
    has_torch = True
except ImportError:
    has_torch = False

# âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì •ì˜
TEST_DATA = [
    {
        "query": "í•œê¸€ì„ ë§Œë“  ì‚¬ëŒì€ ëˆ„êµ¬ì¸ê°€?",
        "documents": [
            "í•œê¸€ì€ ì„¸ì¢…ëŒ€ì™•ì´ ì°½ì œí•˜ì˜€ë‹¤.",
            "í•œê¸€ì€ í•œêµ­ì˜ ë¬¸ìì´ë‹¤.",
            "ì„¸ì¢…ëŒ€ì™•ì€ ì¡°ì„ ì˜ ì™•ì´ë‹¤."
        ],
        "expected": "í•œê¸€ì€ ì„¸ì¢…ëŒ€ì™•ì´ ì°½ì œí•˜ì˜€ë‹¤.",
        "label_scores": [1.0, 0.4, 0.7]
    },
    {
        "query": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€?",
        "documents": [
            "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤.",
            "ì„œìš¸ì€ í•œêµ­ì˜ ë„ì‹œì´ë‹¤.",
            "ë¶€ì‚°ì€ ëŒ€í•œë¯¼êµ­ì˜ ë„ì‹œì´ë‹¤."
        ],
        "expected": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤.",
        "label_scores": [1.0, 0.8, 0.6]
    }
]

# âœ… ëª¨ë¸ ëª©ë¡ ì •ì˜
HF_MODELS = {
    "snowflake-ko": "dragonkue/snowflake-arctic-embed-l-v2.0-ko",
    "bge-m3-ko": "dragonkue/BGE-m3-ko",
    "e5-small-ko": "dragonkue/multilingual-e5-small-ko",
    "kosimcse": "BM-K/KoSimCSE-roberta-multitask"
}

OLLAMA_MODELS = {
    "granite": "granite-embedding:278m",
    "nomic": "nomic-embed-text:latest"
}


def evaluate_similarity(model, is_ollama=False):
    all_corr_spearman = []
    all_corr_pearson = []

    for data in TEST_DATA:
        query = data["query"]
        docs = data["documents"]
        human_scores = data["label_scores"]

        if is_ollama:
            query_vec = model.embed_query(query)
            doc_vecs = model.embed_documents(docs)
        else:
            query_vec = model.encode([query])[0]
            doc_vecs = model.encode(docs)

        sims = cosine_similarity([query_vec], doc_vecs)[0]
        spearman_corr = spearmanr(sims, human_scores).correlation
        pearson_corr = pearsonr(sims, human_scores)[0]

        all_corr_spearman.append(spearman_corr)
        all_corr_pearson.append(pearson_corr)

    return np.mean(all_corr_spearman), np.mean(all_corr_pearson)


def evaluate_retrieval(model, is_ollama=False):
    correct = 0
    total = 0

    for data in TEST_DATA:
        query = data["query"]
        docs = data["documents"]
        expected = data["expected"]

        if is_ollama:
            query_vec = model.embed_query(query)
            doc_vecs = model.embed_documents(docs)
        else:
            query_vec = model.encode([query])[0]
            doc_vecs = model.encode(docs)

        sims = cosine_similarity([query_vec], doc_vecs)[0]
        retrieved = docs[np.argmax(sims)]

        if retrieved == expected:
            correct += 1
        total += 1

    return correct / total


def run_all_tests():
    results = []

    for name, path in HF_MODELS.items():
        print(f"ğŸ” HuggingFace: {name}")
        model = SentenceTransformer(path, device="cuda")

        acc = evaluate_retrieval(model)
        sp, pe = evaluate_similarity(model)
        results.append({"model": name, "rag_accuracy": acc, "spearman": sp, "pearson": pe})

        del model
        gc.collect()
        if has_torch:
            torch.cuda.empty_cache()

    for name, path in OLLAMA_MODELS.items():
        print(f"ğŸ” Ollama: {name}")
        model = OllamaEmbeddings(model=path)

        acc = evaluate_retrieval(model, is_ollama=True)
        sp, pe = evaluate_similarity(model, is_ollama=True)
        results.append({"model": name, "rag_accuracy": acc, "spearman": sp, "pearson": pe})

        del model
        gc.collect()
        if has_torch:
            torch.cuda.empty_cache()

    return pd.DataFrame(results)


def visualize_results(df: pd.DataFrame):
    metrics = ["rag_accuracy", "spearman", "pearson"]
    fig, axs = plt.subplots(len(metrics), 1, figsize=(10, 12))

    for i, metric in enumerate(metrics):
        axs[i].bar(df["model"], df[metric])
        axs[i].set_title(metric)
        axs[i].set_ylim(0, 1.05)
        axs[i].set_ylabel("Score")
        axs[i].grid(True)

    plt.tight_layout()
    plt.savefig("embedding_model_evaluation.png")
    print("âœ… ì €ì¥ ì™„ë£Œ: embedding_model_evaluation.png")


if __name__ == "__main__":
    df_result = run_all_tests()
    visualize_results(df_result)
